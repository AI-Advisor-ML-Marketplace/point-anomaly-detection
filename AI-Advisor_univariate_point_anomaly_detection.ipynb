{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, tune, and deploy a custom ML model using <font color='red'>AI-Advisor Univariate Point Anomaly Detection </font> Algorithm from AWS Marketplace \n",
    "\n",
    "\n",
    "<font color='red'> This algorithm detects anomaly points for continuous univariate time series data that peaks up suddenly, using unsupervised Machine Learning Anomaly Detection approach. </font>\n",
    "\n",
    "This sample notebook shows you how to train a custom ML model using <font color='red'> For Seller to update: [AI-Advisor UPAD](https://aws.amazon.com/marketplace/pp/prodview-sokpwxdi3qrb4?sr=0-1&ref_=beagle&applicationId=AWSMPContessa)</font> from AWS Marketplace.\n",
    "\n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook.\n",
    "\n",
    "## Pre-requisites\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. Some hands-on experience using [Amazon SageMaker](https://aws.amazon.com/sagemaker/).\n",
    "1. To use this algorithm successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to <font color='red'> For Seller to update: [AI-Advisor UPAD](https://github.com/AI-Advisor-ML-Marketplace/point-anomaly-detection)</font>. \n",
    "\n",
    "## Contents\n",
    "1. [Subscribe to the algorithm](#1.-Subscribe-to-the-algorithm)\n",
    "2. [Prepare dataset](#2.-Prepare-dataset)\n",
    "\t1. [Dataset format expected by the algorithm](#A.-Dataset-format-expected-by-the-algorithm)\n",
    "\t2. [Configure and visualize train and test dataset](#B.-Configure-and-visualize-train-and-test-dataset)\n",
    "\t3. [Upload datasets to Amazon S3](#C.-Upload-datasets-to-Amazon-S3)\n",
    "3. [Train a machine learning model](#3:-Train-a-machine-learning-model)\n",
    "\t1. [Set up environment](#3.1-Set-up-environment)\n",
    "\t2. [Train a model](#3.2-Train-a-model)\n",
    "4. [Deploy model and verify results](#4:-Deploy-model-and-verify-results)\n",
    "    1. [Deploy trained model](#A.-Deploy-trained-model)\n",
    "    2. [Create input payload](#B.-Create-input-payload)\n",
    "    3. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "    4. [Visualize output](#D.-Visualize-output)\n",
    "    6. [Delete the endpoint](#F.-Delete-the-endpoint)\n",
    "5. [Perform Batch inference](#5.-Perform-Batch-inference)\n",
    "6. [Clean-up](#6.-Clean-up)\n",
    "\t1. [Delete the model](#A.-Delete-the-model)\n",
    "\n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the algorithm:\n",
    "1. Open the algorithm listing page <font color='red'> For Seller to update: [AI-Advisor UPAD](https://aws.amazon.com/marketplace/pp/prodview-sokpwxdi3qrb4?sr=0-1&ref_=beagle&applicationId=AWSMPContessa)</font>\n",
    "1. On the AWS Marketplace listing,  click on **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you agree with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn**. This is the algorithm ARN that you need to specify while training a custom ML model. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![product_arn_image](images/product_arn_image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from getpass import getpass \n",
    "\n",
    "# SHAPE\n",
    "# algo_arn = \"<Customer to specify algorithm ARN corresponding to their AWS region follow the instruction above>\"\n",
    "\n",
    "########################################CHANGE####################################################\n",
    "# SAMPLE\n",
    "algo_arn='arn:aws:sagemaker:us-east-2:438613450817:algorithm/aiadvisor-pad-v1-1-5'\n",
    "##################################################################################################\n",
    "\n",
    "# get your seesion information\n",
    "#####################################################\n",
    "aws_region = \"us-east-2\"  ##\n",
    "aws_access_key = getpass(prompt=\"Access key: \")\n",
    "aws_secret_key = getpass(prompt=\"Secret key: \")\n",
    "######################aws_access_key#########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "import uuid\n",
    "from sagemaker import ModelPackage\n",
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import ModelPackage\n",
    "from urllib.parse import urlparse\n",
    "import boto3\n",
    "from IPython.display import Image\n",
    "from PIL import Image as ImageEdit\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Dataset format expected by the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution follows these **2 steps**:  `Training` and `Testing` the algorithm.\n",
    "\n",
    "**Train**\n",
    "- The algorithm trains on user provided dataset.\n",
    "- Dataset must be in `txt/csv` shape, under `./data/train/` folder, with 'utf-8' encoding.\n",
    "\n",
    "**Test**\n",
    "- After the Machine Learning model is trained, it can be used to make prediction using test dataset.\n",
    "- The algorithm also tests on user provided dataset.\n",
    "- Dataset must be in `txt/csv` shape, under `./data/test/` folder, with 'utf-8' encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### B. Configure and visualize train and test dataset\n",
    "The `train` and `test` dataset should look like this as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd # import padas to show how data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SHAPE\n",
    "# training_dataset = \"data/train/<FileName.ext>\"\n",
    "\n",
    "########################################CHANGE####################################################\n",
    "# SAMPLE\n",
    "training_dataset = \"data/training/train.csv\"\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show sample of training dataset\n",
    "df = pd.read_csv(training_dataset)\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SHAPE\n",
    "# test_dataset = \"data/test/<FileName.ext>\"\n",
    "\n",
    "########################################CHANGE####################################################\n",
    "# SAMPLE\n",
    "test_dataset = \"data/inference/test.csv\"\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show sample of test dataset\n",
    "df = pd.read_csv(test_dataset)\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Upload datasets to Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Do not change bucket parameter value. Do not hardcode your S3 bucket name.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "boto_session = boto3.Session(region_name=aws_region, aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key)\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session) # get session info\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload training data to s3 bucket\n",
    "algo_prefix = \"point-anomaly-detection\"\n",
    "training_data = sagemaker_session.upload_data(training_dataset, bucket=bucket, key_prefix=algo_prefix + \"/traing-input-data\")\n",
    "print(\"Training input uploaded to : \" + training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload test data to s3 bucket\n",
    "test_data = sagemaker_session.upload_data(test_dataset, bucket=bucket, key_prefix=algo_prefix+\"/inference-input-data\")\n",
    "print(\"Inference input uploaded to : \" + test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Train a machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that dataset is available in an accessible Amazon S3 bucket, we are ready to train a machine learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## If you are running on a local server, enter the role name specified in IAM role.\n",
    "\n",
    "sts = boto3.client('sts', region_name=aws_region, aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key)\n",
    "caller_identity = sts.get_caller_identity()\n",
    "account_id = caller_identity['Account']\n",
    "role_name = input(\"Role name: \")\n",
    "role = f'arn:aws:iam::{account_id}:role/{role_name}'\n",
    "\n",
    "\n",
    "\n",
    "### If you are running in sagemaker jupyter notebook then uncomment the below. (The above is commented out.) \n",
    "\n",
    "#from sagemaker import get_execution_role\n",
    "#role = get_execution_role(sagemaker_session=sagemaker_session)\n",
    "\n",
    "print (f\"Result: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>For Seller to update: update algorithm sepcific unique prefix in following cell. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SHAPE\n",
    "# output_location = \"s3://{}/<For seller to Update:Update a unique prefix>/{}\".format(bucket, \"output\")\n",
    "\n",
    "########################################CHANGE####################################################\n",
    "# SAMPLE\n",
    "output_location = \"s3://{}/ai-advisor-upad/{}\".format(bucket, \"output\")\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find more information about dataset format in **Hyperparameters** section of <font color='red'> For Seller to update: [AI-Advisor UPAD](https://aws.amazon.com/marketplace/pp/prodview-sokpwxdi3qrb4?sr=0-1&ref_=beagle&applicationId=AWSMPContessa).</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Parameters are divided into mandatory input parts and optional input parts.  \n",
    "- x_column: Enter the column name to be diagnosed.\n",
    "- time_column: Enter the name of the time column.\n",
    "- index_column: Enter the column name to specify groupby for x_column. Create different models for each group.   \n",
    "  (Supports up to 3 columns. Enter in string form without spaces, such as \"col1, col2, col3\".)\n",
    " \n",
    " Below are optional parameters.\n",
    "- hpo_repeat: The number of executions of bayesian optimization to find the optimal parameters. (type: integer)\n",
    "- decision_rule: Choose one among \"two\", \"upper\", and \"lower\".  \n",
    "  Select \"upper\" if you want to detect only anomaly points that occur above the center of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################CHANGE####################################################\n",
    "# Define hyperparameters\n",
    "hyperparameters = {\n",
    "            'x_columns': 'value',\n",
    "            'time_column': 'timestamp',\n",
    "            'index_columns': '', ## Leave blank if groupby is not required\n",
    "    \n",
    "            'hpo_repeat': 3,                 \n",
    "            'decision_rule': 'upper'} ##upeer, lower, two\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>For Seller to update: Update appropriate values in estimator definition and ensure that fit call works as expected.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For information on creating an `Estimator` object, see [documentation](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################CHANGE####################################################\n",
    "# Create an estimator object for running a training job\n",
    "estimator = sagemaker.algorithm.AlgorithmEstimator(\n",
    "    algorithm_arn=algo_arn,\n",
    "    base_job_name=\"ai-advisor-upad\",\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.xlarge',\n",
    "    input_mode=\"File\",\n",
    "    output_path=output_location,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "##################################################################################################\n",
    "\n",
    "# Run the training job.\n",
    "estimator.fit({'training': training_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See this [blog-post](https://aws.amazon.com/blogs/machine-learning/easily-monitor-and-visualize-metrics-while-training-models-on-amazon-sagemaker/) for more information how to visualize metrics during the process. You can also open the training job from [Amazon SageMaker console](https://console.aws.amazon.com/sagemaker/home?#/jobs/) and monitor the metrics/logs in **Monitor** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Deploy model and verify results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can deploy the model for performing real-time inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################CHANGE####################################################\n",
    "model_name = \"ai-advisor-upad\"\n",
    "content_type='text/csv'\n",
    "\n",
    "# set instance type\n",
    "instance_type = 'ml.c5.2xlarge'\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Deploy trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "# deploy model\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type= instance_type, \n",
    "    serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint is created, you can perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Create input payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_name = \"data/inference/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Add code snippet that shows the payload contents>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference result is delivered as a tar file, which contains two csv files. One CSV file is a form in which diagnosis results are included in the input data, and the other is a summary file that collects only rows with anomaly points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "runtime = boto3.client('sagemaker-runtime', region_name=aws_region, aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key)\n",
    "\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=predictor.endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    #Body=file_name.encode('utf-8'),\n",
    "    Body=open(file_name, 'rb').read(),\n",
    "    Accept=content_type\n",
    ")\n",
    "\n",
    "content = response['Body'].read()\n",
    "binary_stream = io.BytesIO(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "with tarfile.open(fileobj=binary_stream, mode='r') as tar:\n",
    "    csv_contents = list()\n",
    "    image_contents = list()\n",
    "    for member in tar.getmembers():\n",
    "        if member.name.endswith('.csv'):\n",
    "            csv_contents.append(tar.extractfile(member).read())\n",
    "        elif member.name.endswith('.png'):\n",
    "            image_contents.append(tar.extractfile(member).read())\n",
    "\n",
    "if len(csv_contents) != 0 :\n",
    "    for cnt, csv_raw in enumerate(csv_contents):\n",
    "        df = pd.read_csv(io.StringIO(csv_raw.decode('utf-8')))\n",
    "        display(df.tail(10))\n",
    "        if cnt == 0:\n",
    "            result_df = df\n",
    "        elif cnt == 1:\n",
    "            summary_df = df\n",
    "\n",
    "        \n",
    "if len(image_contents) != 0 : \n",
    "    for img_raw in image_contents:\n",
    "        img = Image.open(io.BytesIO(png_contents))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "colors = plt.rcParams[\"axes.prop_cycle\"]()\n",
    "markers = list(mk.MarkerStyle.markers.keys())[:31]\n",
    "\n",
    "##Set Display\n",
    "fig, ax = plt.subplots(figsize = (20,5))\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Value')\n",
    "\n",
    "## Display main graph\n",
    "disp_df = result_df[hyperparameters[\"x_columns\"]].copy()\n",
    "disp_df.plot(ax=ax, linestyle = \"none\", marker= 'o', markersize = 3, alpha = 0.7, c= 'gray')\n",
    "\n",
    "ax.scatter(summary_df[\"index\"], summary_df[hyperparameters[\"x_columns\"]], color='orange', alpha=0.7, s=300, marker='*',\n",
    "           zorder=cnt, label= \"anomaly_point\")\n",
    "\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Delete the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. you can terminate the same to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is an experiment, you do not need to run a hyperparameter tuning job. However, if you would like to see how to tune a model trained using a third-party algorithm with Amazon SageMaker's hyperparameter tuning functionality, you can run the optional tuning step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>For seller to update: Review/update the tuner configuration including but not limited to `base_tuning_job_name`, `max_jobs`, and `max_parallel_jobs`. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(\n",
    "    estimator=estimator,\n",
    "    base_tuning_job_name=\"<For Seller to update: Specify base job name>\",\n",
    "    objective_metric_name=objective_metric_name,\n",
    "    objective_type=tuning_direction,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    max_jobs=50,\n",
    "    max_parallel_jobs=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>For seller to update: Uncomment following lines, specify appropriate channels, and run the tuner to test it out. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment following two lines to run Hyperparameter optimization job.\n",
    "# tuner.fit({'training':  data})\n",
    "# tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>For seller to update: Once you have tested the code written in the preceding cell, comment three lines in the preceding cell so that customers who choose to simply run entire notebook do not end up triggering a tuning job. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have completed a tuning job, (or even while the job is still running) you can [clone and use this notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/analyze_results/HPO_Analyze_TuningJob_Results.ipynb) to analyze the results to understand how each hyperparameter effects the quality of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Perform Batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will perform batch inference using multiple input payloads together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################CHANGE####################################################\n",
    "# upload the batch-transform job input files to S3\n",
    "transform_dataset = \"data/inference/test.csv\"\n",
    "##################################################################################################\n",
    "\n",
    "transform_input = sagemaker_session.upload_data(transform_dataset, key_prefix=model_name)\n",
    "print(\"Transform input uploaded to : \" + transform_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the batch-transform job\n",
    "transformer = estimator.transformer(instance_count=1, instance_type=instance_type)\n",
    "transformer.transform(transform_input, content_type=content_type)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output is available on following path\n",
    "transformer.output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
